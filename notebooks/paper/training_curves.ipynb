{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from retro_branching.utils import get_most_recent_checkpoint_foldername, PlotAesthetics\n",
    "\n",
    "import glob\n",
    "import gzip\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import math\n",
    "from ordered_set import OrderedSet\n",
    "import time\n",
    "from sqlitedict import SqliteDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 100, \"display.max_columns\", None)\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 100x100 RL\n",
    "# agent_to_path = {'RL': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1376/database/'}\n",
    "\n",
    "# 500x1000 RL\n",
    "# agent_to_path = {'RL': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1236/database/'}\n",
    "\n",
    "# 500x1000 ablating sub-trees\n",
    "agent_to_path = {'Full': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1405/database/',\n",
    "                 'Retrospective': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1236/database/',\n",
    "                 'Fathomed': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1464/database/'}\n",
    "# agent_to_path = {'No sub-trees': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1405/database/'}\n",
    "# agent_to_path = {'No sub-trees': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1412/database/'}\n",
    "\n",
    "# 500x1000 Etheve et al. baseline\n",
    "agent_to_path = {'Step-Orig': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1405/database/',\n",
    "                 'Step-Retro': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1236/database/',\n",
    "                 'Etheve': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1471/database/',\n",
    "                 'Etheve-Orig': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1471/database/'}\n",
    "#                  'Etheve-Retro': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1472/database/'}\n",
    "agent_to_path = {\n",
    "#                  'Step-Orig': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1479/database/',\n",
    "                 'Step-Retro': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1481/database/',\n",
    "                 'Etheve': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1484/database/',\n",
    "                 'MAB-Retro': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1485/database/',\n",
    "                }\n",
    "\n",
    "# # 100x100 ablating sub-trees\n",
    "# agent_to_path = {'No sub-trees': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1397/database/',\n",
    "#                  'Sub-trees': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1376/database/'}\n",
    "\n",
    "# agent_to_path = {'No sub-trees': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1412/database/',\n",
    "#                  'Sub-trees': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1411/database/'}\n",
    "\n",
    "\n",
    "# agent_to_path = {'DQN': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1437/database/',\n",
    "#                  'n-step DQN': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1438/database/',\n",
    "#                  'M-DQN': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1439/database/',\n",
    "#                  'n-step M-DQN': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1436/database/'}\n",
    "\n",
    "\n",
    "\n",
    "# # 250x500 ablating sub-trees\n",
    "# agent_to_path = {'Full': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1440/database/',\n",
    "#                  'Retrospective': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1436/database/',\n",
    "# #                  'Retrospective': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1437/database/',\n",
    "#                 }\n",
    "\n",
    "# # 1000x1000 \n",
    "# agent_to_path = {'DQN': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1448/database/',\n",
    "#                  'n-step M-DQN': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1450/database/',\n",
    "# #                  'Retrospective': '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_1437/database/',\n",
    "#                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load agent logs\n",
    "episodes_logs_dict, epochs_logs_dict = {}, {}\n",
    "agent_name_to_display_name = {}\n",
    "for display_name, path in agent_to_path.items():\n",
    "    print(f'\\nLoading logs for {display_name} agent from {path}...')\n",
    "    start_t = time.time()\n",
    "    episodes_logs_dict[display_name], epochs_logs_dict[display_name] = {}, {}\n",
    "    while True:\n",
    "        try:\n",
    "            with SqliteDict(path+'episodes_log.sqlite') as log:\n",
    "                for key, val in log.items():\n",
    "                    # read into memory\n",
    "                    episodes_logs_dict[display_name][key] = val\n",
    "                log.close()\n",
    "            break\n",
    "        except:\n",
    "            # database locked since is being written to, wait and try again\n",
    "            time.sleep(1)\n",
    "    print(f'Loaded {display_name} agent episodes log in {time.time() - start_t:.3f} s')\n",
    "    start_t = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            with SqliteDict(path+'epochs_log.sqlite') as log:\n",
    "                for key, val in log.items():\n",
    "                    # read into memory\n",
    "                    epochs_logs_dict[display_name][key] = val\n",
    "                log.close()\n",
    "                break\n",
    "        except:\n",
    "            # database locked since is being written to, wait and try again\n",
    "            time.sleep(1)\n",
    "    print(f'Loaded {display_name} agent epochs log in {time.time() - start_t:.3f} s')\n",
    "print(f'\\nAll agent logs loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = defaultdict(list)\n",
    "for agent, log in episodes_logs_dict.items():\n",
    "    print(log.keys())\n",
    "    \n",
    "    # yvals\n",
    "    _df['num_nodes'].extend(log['num_nodes'])\n",
    "    _df['lp_iterations'].extend(log['lp_iterations'])\n",
    "#     _df['reward'].extend(log['reward'])\n",
    "    _df['R'].extend(log['R'])\n",
    "    \n",
    "    # xvals\n",
    "    if 'num_episodes' not in log:\n",
    "        print(f'num_episodes not found in agent {agent} log, estimating...')\n",
    "        _df['num_episodes'].extend([ep for ep in range(log['episode_counter']+1)])\n",
    "    else:\n",
    "        _df['num_episodes'].extend(log['num_episodes'])\n",
    "    if 'num_epochs' not in log:\n",
    "        print(f'num_epochs not found in agent {agent} log, estimating...')\n",
    "        mean_epochs_per_ep = int((log['epoch_counter']+1) / (log['episode_counter']+1))\n",
    "        print([(ep+1)*mean_epochs_per_ep for ep in range(log['episode_counter']+1)])\n",
    "        _df['num_epochs'].extend([(ep+1)*mean_epochs_per_ep for ep in range(log['episode_counter']+1)])\n",
    "    else:\n",
    "        _df['num_epochs'].extend(log['num_epochs'])\n",
    "    if 'num_actor_steps' not in log:\n",
    "        print(f'num_actor_steps not found in agent {agent} log, estimating...')\n",
    "        total_steps = np.sum(log['num_steps'])\n",
    "        mean_steps_per_ep = int(total_steps / (log['episode_counter']+1))\n",
    "        _df['num_actor_steps'].extend([(ep+1)*mean_steps_per_ep for ep in range(log['episode_counter']+1)])\n",
    "    else:\n",
    "        _df['num_actor_steps'].extend(log['num_actor_steps'])\n",
    "    \n",
    "    print(len(_df['num_nodes']))\n",
    "    print(len(_df['num_episodes']))\n",
    "    print(len(_df['num_epochs']))\n",
    "    print(len(_df['num_actor_steps']))\n",
    "    \n",
    "#     print(_df['num_nodes'])\n",
    "#     print(_df['num_epochs'])\n",
    "    \n",
    "    # labels\n",
    "    _df['Agent'].extend([agent for _ in range(len(log['num_nodes']))])\n",
    "    \n",
    "df = pd.DataFrame(_df)\n",
    "# print(df[:10])\n",
    "# print(df)\n",
    "display(df[:10])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = 'num_epochs' # 'num_episodes' 'num_epochs' 'num_actor_steps'\n",
    "\n",
    "# rolling_window = None\n",
    "# rolling_window = int(5)\n",
    "# rolling_window = int(10)\n",
    "# rolling_window = int(50)\n",
    "# rolling_window = int(100)\n",
    "# rolling_window = int(150)\n",
    "# rolling_window = int(300)\n",
    "# rolling_window = int(500) # paper training curve\n",
    "rolling_window = int(1e3)\n",
    "# rolling_window = int(5e3)\n",
    "# rolling_window = int(10e3)\n",
    "\n",
    "xlog = False # False True\n",
    "ylog = True # False True\n",
    "\n",
    "xaxis_label_style = 'sci' # paper training curve\n",
    "yaxis_label_style = 'plain'\n",
    "\n",
    "xlim = None\n",
    "# xlim = [0, 2e5] # paper training curve\n",
    "# xlim = [0, 1e5]\n",
    "# xlim = [0, 1.5e5]\n",
    "xlim = [0, 2.3e5]\n",
    "ylim = None\n",
    "\n",
    "# legend = False # 'auto' False\n",
    "legend = 'auto'\n",
    "\n",
    "# scaling_factor = 0.6 # paper training curve\n",
    "# scaling_factor = 2.5\n",
    "scaling_factor = 0.4\n",
    "width_scaling_factor = 1\n",
    "height_scaling_factor = 1\n",
    "\n",
    "aesthetics = PlotAesthetics()\n",
    "# aesthetics.set_icml_paper_plot_aesthetics() # paper training curve\n",
    "aesthetics.set_icml_paper_plot_aesthetics(font_scale=0.4, linewidth=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Original df:')\n",
    "display(df[:15])\n",
    "display(df)\n",
    "display(df[-15:])\n",
    "\n",
    "# # # OLD\n",
    "# _new_df = copy.deepcopy(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NEW NEW\n",
    "# for each x val, repeat y val with some minimum resolution until next x val encountered to fill out 'missing' data\n",
    "min_res = 10\n",
    "\n",
    "# convert dict vals from idx -> val dict to val list\n",
    "new_df_dict = copy.deepcopy(df).to_dict()\n",
    "for key, val in new_df_dict.items():\n",
    "    new_df_dict[key] = list(val.values())\n",
    "\n",
    "# perform x-val interpolation\n",
    "num_insertions = 0\n",
    "for idx, val in enumerate(df[xaxis][:-1]):\n",
    "    diff = df[xaxis].iloc[idx+1] - df[xaxis].iloc[idx]\n",
    "    num_increments = int(diff / min_res)\n",
    "    print(f'\\norig idx {idx} val: {val} | idx+1 val: {df[xaxis].iloc[idx+1]} | diff: {diff} | num_increments: {num_increments}')\n",
    "    if diff != min_res and num_increments != 0:\n",
    "        for i in range(num_increments):\n",
    "            print(f'i: {i} -> insert val {val+((i+1)*min_res)} at idx {idx+i+num_insertions+1}')\n",
    "            new_df_dict[xaxis].insert(idx+i+num_insertions+1, val+((i+1)*min_res))\n",
    "            for column in df.columns:\n",
    "                if column != xaxis:\n",
    "                    new_df_dict[column].insert(idx+i+num_insertions+1, df[column][idx])\n",
    "    else:\n",
    "        # no need to interpolate to next x val\n",
    "        print('no interpolation needed')\n",
    "    num_insertions += num_increments\n",
    "interpolated_df = pd.DataFrame(new_df_dict)\n",
    "print(f'\\ninterpolated_df with interpolated x vals:')\n",
    "display(interpolated_df[:15])\n",
    "display(interpolated_df)\n",
    "_new_df = copy.deepcopy(interpolated_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NEW\n",
    "# remove any duplicate x-values to prevent NaNs when using pd.rolling()\n",
    "new_df = defaultdict(list)\n",
    "for agent in agent_to_path.keys():\n",
    "# for agent in ['Retrospective']:\n",
    "    print(f'\\nAgent: {agent}')\n",
    "    # select agent's values\n",
    "    _agent_df = interpolated_df.loc[interpolated_df['Agent'] == agent]\n",
    "    print(f'Agent df indexed by agent:\\n{_agent_df[:15]}')\n",
    "    \n",
    "    # average entries of any duplicate x-axis values\n",
    "    agent_df = _agent_df.groupby(xaxis, as_index=False).mean()\n",
    "#     agent_df = _agent_df.drop_duplicates(subset=[xaxis], keep='first')\n",
    "    print(f'Agent df averaged over duplicate {xaxis} values:\\n{agent_df[:15]}')\n",
    "    \n",
    "    # put agent labels back into agent dataframe\n",
    "    agent_df['Agent'] = [agent for _ in range(len(agent_df))]\n",
    "    print(f'Agent df with {agent} rows re-applied:\\n{agent_df[:15]}')\n",
    "    \n",
    "#     sns.lineplot(data=agent_df, x='num_epochs', y='num_nodes')\n",
    "#     plt.show()\n",
    "    \n",
    "    for key, col in agent_df.to_dict().items():\n",
    "        print(key)\n",
    "        vals = list(col.values())\n",
    "        new_df[key].extend(vals)\n",
    "_new_df = pd.DataFrame(new_df)\n",
    "print('New df with deleted repeat values')\n",
    "display(_new_df[:15])\n",
    "display(_new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc rolling averages\n",
    "new_df = copy.deepcopy(_new_df)\n",
    "if rolling_window is not None:\n",
    "    for param in ['num_nodes', 'lp_iterations', 'R']:\n",
    "#         new_df[param] = _new_df[param].rolling(rolling_window, center=False).mean()\n",
    "#         new_df[param] = _new_df[param].rolling(rolling_window, center=False).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "        new_df[param] = _new_df[param].rolling(rolling_window, center=False).mean()\n",
    "    print(f'\\nNew DF after applying rolling average:')\n",
    "    display(new_df[:15])\n",
    "    display(new_df)\n",
    "    display(new_df[-15:])\n",
    "\n",
    "# xaxis = 'num_episodes' # TEMPORARY\n",
    "    \n",
    "if xaxis == 'num_episodes':\n",
    "    xlabel = 'Episodes'\n",
    "elif xaxis == 'num_epochs':\n",
    "    xlabel = 'Learner Steps' # paper training curve\n",
    "    xlabel = 'Epochs'\n",
    "elif xaxis == 'num_actor_steps':\n",
    "    xlabel = 'Actor Steps'\n",
    "else:\n",
    "    raise Exception(f'Have not yet implemented xlabel for xaxis {xaxis}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=aesthetics.get_standard_fig_size(scaling_factor=scaling_factor, width_scaling_factor=width_scaling_factor, height_scaling_factor=height_scaling_factor))\n",
    "g = sns.lineplot(data=new_df, x=xaxis, y='num_nodes', hue='Agent', linewidth=aesthetics.linewidth, legend=legend)\n",
    "if xlim is not None:\n",
    "    plt.xlim(left=xlim[0], right=xlim[1])\n",
    "if ylim is not None:\n",
    "    plt.ylim(bottom=ylim[0], top=ylim[1])\n",
    "# plt.legend(loc='upper right')\n",
    "# ax.legend(loc='lower left', bbox_to_anchor=(0, 1.02, 1, 0.2), ncol=2, prop={'size': 3})\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=3, prop={'size': 3})\n",
    "g.set_xlabel(xlabel)\n",
    "g.set_ylabel('Nodes')\n",
    "plt.ticklabel_format(style=xaxis_label_style, axis='x', scilimits=(0,0))\n",
    "plt.ticklabel_format(style=yaxis_label_style, axis='y', scilimits=(0,0))\n",
    "ax.tick_params(axis='both', which='major', pad=2)\n",
    "ax.xaxis.labelpad = 2\n",
    "ax.yaxis.labelpad = 2\n",
    "sns.despine(ax=ax) # remove top and right spines\n",
    "if xlog:\n",
    "    g.set(xscale='log')\n",
    "if ylog:\n",
    "    g.set(yscale='log')\n",
    "plt.gcf().patch.set_alpha(0.0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=aesthetics.get_standard_fig_size(scaling_factor=scaling_factor, width_scaling_factor=width_scaling_factor, height_scaling_factor=height_scaling_factor))\n",
    "g = sns.lineplot(data=new_df, x=xaxis, y='lp_iterations', hue='Agent', linewidth=aesthetics.linewidth, legend=legend)\n",
    "if xlim is not None:\n",
    "    plt.xlim(left=xlim[0], right=xlim[1])\n",
    "if ylim is not None:\n",
    "    plt.ylim(bottom=ylim[0], top=ylim[1])\n",
    "g.set_xlabel(xlabel)\n",
    "g.set_ylabel('LP Iterations')\n",
    "plt.ticklabel_format(style=xaxis_label_style, axis='x', scilimits=(0,0))\n",
    "plt.ticklabel_format(style=yaxis_label_style, axis='y', scilimits=(0,0))\n",
    "ax.tick_params(axis='both', which='major', pad=2)\n",
    "ax.xaxis.labelpad = 2\n",
    "ax.yaxis.labelpad = 2\n",
    "sns.despine(ax=ax) # remove top and right spines\n",
    "if xlog:\n",
    "    g.set(xscale='log')\n",
    "if ylog:\n",
    "    g.set(yscale='log')\n",
    "plt.gcf().patch.set_alpha(0.0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=aesthetics.get_standard_fig_size(scaling_factor=scaling_factor, width_scaling_factor=width_scaling_factor, height_scaling_factor=height_scaling_factor))\n",
    "g = sns.lineplot(data=new_df, x=xaxis, y='R', hue='Agent', linewidth=aesthetics.linewidth, legend=legend)\n",
    "plt.ticklabel_format(style=xaxis_label_style, axis='x', scilimits=(0,0))\n",
    "plt.ticklabel_format(style=yaxis_label_style, axis='y', scilimits=(0,0))\n",
    "ax.tick_params(axis='both', which='major', pad=2)\n",
    "ax.xaxis.labelpad = 2\n",
    "ax.yaxis.labelpad = 2\n",
    "sns.despine(ax=ax) # remove top and right spines\n",
    "if xlim is not None:\n",
    "    plt.xlim(left=xlim[0], right=xlim[1])\n",
    "if ylim is not None:\n",
    "    plt.ylim(bottom=ylim[0], top=ylim[1])\n",
    "g.set_xlabel(xlabel)\n",
    "g.set_ylabel('Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note to self\n",
    "\n",
    "Sometimes when plot e.g. num_nodes vs. learner_steps, may get a large spike with the sub-trees approach. This occurs because the actor encounters a particularly long episode (e.g. usually actor has num_nodes=50, but then encounters episode with num_nodes=10,000, num_steps=5,000). At end of epsiode, sub-trees are constructed and added to buffer, and learner will then perform num_steps/num_steps_per_update learner steps. So in this example, if num_steps_per_update=5, the learner will perform 1,000 learner steps *before gathering any more experiences*, so num nodes will be fixed at this high number for multiple learner steps/epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
